{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import numpy as np\n",
    "# from PIL import Image\n",
    "\n",
    "# # Function to load and preprocess the images\n",
    "# def load_images_from_folder(folder_path, target_size=(200, 200)):\n",
    "#     images = []\n",
    "#     for filename in os.listdir(folder_path):\n",
    "#         img_path = os.path.join(folder_path, filename)\n",
    "#         try:\n",
    "#             # Open image and resize it\n",
    "#             img = Image.open(img_path).convert('RGB')\n",
    "#             img = img.resize(target_size)  # Resizing the image to a target size\n",
    "#             img_array = np.array(img)  # Converting to NumPy array\n",
    "#             images.append(img_array)\n",
    "#         except Exception as e:\n",
    "#             print(f\"Failed to process {img_path}: {e}\")\n",
    "    \n",
    "#     images = np.array(images)  # Convert list of images to a NumPy array\n",
    "#     return images\n",
    "\n",
    "# # Specify folder path and target image size\n",
    "# folder_path = r\"C:\\Users\\farel\\Documents\\Hology UB\\data\\HOLOGY 2024\\processed\"\n",
    "# images_data = load_images_from_folder(folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "\n",
    "# Function to load images using the 'id' from the CSV\n",
    "def load_images_from_ids(folder_path, ids, target_size=(200, 200)):\n",
    "    images = []\n",
    "    for image_id in ids:\n",
    "        img_filename = f\"{image_id}.jpg\"  # Assuming image filenames are of the format 'id.jpg'\n",
    "        img_path = os.path.join(folder_path, img_filename)\n",
    "        try:\n",
    "            # Open image, resize, and convert to NumPy array\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "            img = img.resize(target_size)\n",
    "            img_array = np.array(img)\n",
    "            images.append(img_array)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load image {img_filename}: {e}\")\n",
    "    \n",
    "    return np.array(images)\n",
    "\n",
    "# Load the CSV file\n",
    "csv_file_path = r\"C:\\Users\\farel\\Documents\\Hology UB\\data\\train.csv\"\n",
    "labels_df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Get the 'id' and 'label' columns from the CSV\n",
    "image_ids = labels_df['id'].values  # The 'id' column from CSV\n",
    "jenis_labels = labels_df['jenis'].values  # The 'label' column from CSV\n",
    "warna_labels = labels_df['warna'].values  # The 'label' column from CSV\n",
    "\n",
    "# Folder where images are stored\n",
    "folder_path = r\"C:\\Users\\farel\\Documents\\Hology UB\\data\\processed\"\n",
    "\n",
    "# Load images based on 'id' column\n",
    "images_data = load_images_from_ids(folder_path, image_ids)\n",
    "# images_data = images_data.astype('float32') / 255.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "# Step 1: Label encode the categorical labels into integers\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(jenis_labels)  # 'labels' should come from your CSV\n",
    "\n",
    "# Step 2: Apply OneHotEncoder to the encoded labels\n",
    "one_hot_encoder = OneHotEncoder(sparse_output=False)\n",
    "one_hot_labels = one_hot_encoder.fit_transform(encoded_labels.reshape(-1, 1))\n",
    "\n",
    "label_encoder2 = LabelEncoder()\n",
    "encoded_labels2 = label_encoder2.fit_transform(warna_labels)  # 'labels' should come from your CSV\n",
    "\n",
    "# Step 2: Apply OneHotEncoder to the encoded labels\n",
    "one_hot_encoder2 = OneHotEncoder(sparse_output=False)\n",
    "one_hot_labels2 = one_hot_encoder2.fit_transform(encoded_labels2.reshape(-1, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Normalize the image data to [0, 1] range\n",
    "images_data = images_data.astype('float32') / 255.0\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train1, y_test1, y_train2, y_test2 = train_test_split(\n",
    "    images_data, one_hot_labels, one_hot_labels2, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert the arrays to TensorFlow tensors\n",
    "X_train = tf.convert_to_tensor(X_train)\n",
    "X_test = tf.convert_to_tensor(X_test)\n",
    "y_train1 = tf.convert_to_tensor(y_train1)\n",
    "y_train2 = tf.convert_to_tensor(y_train2)\n",
    "y_test1 = tf.convert_to_tensor(y_test1)\n",
    "y_test2 = tf.convert_to_tensor(y_test2)\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train1 shape after conversion: (621,)\n",
      "y_train2 shape after encoding: (621, 5)\n"
     ]
    }
   ],
   "source": [
    "# Convert one-hot encoded binary labels to single binary values\n",
    "if y_train1.ndim > 1:  # If y_train1 is one-hot encoded\n",
    "    y_train1 = np.argmax(y_train1, axis=1)  # Convert to binary labels\n",
    "\n",
    "if y_test1.ndim > 1:  # Same for test data\n",
    "    y_test1 = np.argmax(y_test1, axis=1)\n",
    "\n",
    "print(\"y_train1 shape after conversion:\", y_train1.shape)  # Should be (batch_size,)\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# If y_train2 is not already one-hot encoded, one-hot encode it\n",
    "if y_train2.ndim == 1:  # If y_train2 is class indices (shape: (batch_size,))\n",
    "    y_train2 = to_categorical(y_train2, num_classes=5)  # Assuming 5 classes\n",
    "    y_test2 = to_categorical(y_test2, num_classes=5)\n",
    "\n",
    "print(\"y_train2 shape after encoding:\", y_train2.shape)  # Should be (batch_size, num_classes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 829ms/step - loss: 4.7588 - output1_accuracy: 0.5532 - output1_loss: 1.4523 - output2_accuracy: 0.2991 - output2_loss: 3.1871 - val_loss: 1.6450 - val_output1_accuracy: 0.6410 - val_output1_loss: 0.6764 - val_output2_accuracy: 0.7692 - val_output2_loss: 0.7985\n",
      "Epoch 2/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 769ms/step - loss: 1.8915 - output1_accuracy: 0.5629 - output1_loss: 0.7904 - output2_accuracy: 0.6616 - output2_loss: 0.9206 - val_loss: 1.2458 - val_output1_accuracy: 0.6538 - val_output1_loss: 0.6030 - val_output2_accuracy: 0.8910 - val_output2_loss: 0.4558\n",
      "Epoch 3/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 754ms/step - loss: 1.5095 - output1_accuracy: 0.6469 - output1_loss: 0.7071 - output2_accuracy: 0.8079 - output2_loss: 0.6111 - val_loss: 1.0757 - val_output1_accuracy: 0.7821 - val_output1_loss: 0.5189 - val_output2_accuracy: 0.9231 - val_output2_loss: 0.3714\n",
      "Epoch 4/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 763ms/step - loss: 1.3208 - output1_accuracy: 0.6872 - output1_loss: 0.6123 - output2_accuracy: 0.8267 - output2_loss: 0.5201 - val_loss: 0.9316 - val_output1_accuracy: 0.8077 - val_output1_loss: 0.4922 - val_output2_accuracy: 0.9359 - val_output2_loss: 0.2555\n",
      "Epoch 5/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 767ms/step - loss: 1.1541 - output1_accuracy: 0.7068 - output1_loss: 0.5773 - output2_accuracy: 0.8621 - output2_loss: 0.3862 - val_loss: 1.1276 - val_output1_accuracy: 0.8013 - val_output1_loss: 0.4798 - val_output2_accuracy: 0.7628 - val_output2_loss: 0.4549\n",
      "Epoch 6/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 744ms/step - loss: 1.1357 - output1_accuracy: 0.7400 - output1_loss: 0.5455 - output2_accuracy: 0.8623 - output2_loss: 0.3937 - val_loss: 0.8547 - val_output1_accuracy: 0.8526 - val_output1_loss: 0.4419 - val_output2_accuracy: 0.9295 - val_output2_loss: 0.2200\n",
      "Epoch 7/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 768ms/step - loss: 0.9530 - output1_accuracy: 0.7877 - output1_loss: 0.4276 - output2_accuracy: 0.8822 - output2_loss: 0.3287 - val_loss: 0.8263 - val_output1_accuracy: 0.8590 - val_output1_loss: 0.4415 - val_output2_accuracy: 0.9551 - val_output2_loss: 0.1943\n",
      "Epoch 8/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 780ms/step - loss: 0.7986 - output1_accuracy: 0.8312 - output1_loss: 0.3789 - output2_accuracy: 0.9306 - output2_loss: 0.2259 - val_loss: 0.7836 - val_output1_accuracy: 0.8718 - val_output1_loss: 0.4289 - val_output2_accuracy: 0.9359 - val_output2_loss: 0.1721\n",
      "Epoch 9/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 783ms/step - loss: 0.8133 - output1_accuracy: 0.8194 - output1_loss: 0.3772 - output2_accuracy: 0.9080 - output2_loss: 0.2490 - val_loss: 0.7880 - val_output1_accuracy: 0.8590 - val_output1_loss: 0.4167 - val_output2_accuracy: 0.9359 - val_output2_loss: 0.1901\n",
      "Epoch 10/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 805ms/step - loss: 0.8399 - output1_accuracy: 0.8171 - output1_loss: 0.3498 - output2_accuracy: 0.9046 - output2_loss: 0.3038 - val_loss: 0.8490 - val_output1_accuracy: 0.8269 - val_output1_loss: 0.4480 - val_output2_accuracy: 0.9167 - val_output2_loss: 0.2203\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, MaxPooling2D, Input, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "\n",
    "# Input layer\n",
    "input_layer = Input(shape=(200, 200, 3))\n",
    "\n",
    "# Shared convolutional base\n",
    "x = Conv2D(64, (3, 3), activation='relu')(input_layer)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.001))(x)\n",
    "x = Dropout(0.6)(x)  \n",
    "\n",
    "# Output 1: Binary classification\n",
    "output1 = Dense(1, activation='sigmoid', name='output1')(x)\n",
    "\n",
    "# Output 2: Multi-class classification (5 classes)\n",
    "output2 = Dense(5, activation='softmax', name='output2')(x)\n",
    "\n",
    "# Define the model with two outputs\n",
    "model = Model(inputs=input_layer, outputs=[output1, output2])\n",
    "\n",
    "# Compile the model with accuracy as the metric\n",
    "model.compile(optimizer='adam',\n",
    "              loss={'output1': 'binary_crossentropy', 'output2': 'categorical_crossentropy'},\n",
    "              metrics={'output1': 'accuracy', 'output2': 'accuracy'})\n",
    "\n",
    "# Early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2, min_lr=0.00001)\n",
    "\n",
    "\n",
    "\n",
    "# Train the model with both outputs\n",
    "history = model.fit(X_train, {'output1': y_train1, 'output2': y_train2},\n",
    "                    validation_data=(X_test, {'output1': y_test1, 'output2': y_test2}),\n",
    "                    epochs=10, batch_size=32, callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 204ms/step\n",
      "Exact Match Ratio: 0.8269\n"
     ]
    }
   ],
   "source": [
    "# Get predictions from the model on the test data\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Split predictions for both outputs\n",
    "pred_output1 = predictions[0]  # Binary classification predictions\n",
    "pred_output2 = predictions[1]  # Multi-class classification predictions\n",
    "# Convert binary predictions to 0 or 1\n",
    "pred_output1_binary = np.round(pred_output1).astype(int)\n",
    "\n",
    "# Convert multi-class predictions to class indices (e.g., choose the class with the highest probability)\n",
    "pred_output2_classes = np.argmax(pred_output2, axis=1)\n",
    "\n",
    "# Now compare with true labels (already converted earlier)\n",
    "correct_binary = (pred_output1_binary.squeeze() == y_test1).astype(int)\n",
    "correct_multiclass = (pred_output2_classes == np.argmax(y_test2, axis=1)).astype(int)\n",
    "\n",
    "# Exact match: Both binary and multi-class predictions must be correct for each sample\n",
    "exact_matches = (correct_binary & correct_multiclass)\n",
    "\n",
    "# Calculate exact match ratio\n",
    "exact_match_ratio = np.mean(exact_matches)\n",
    "\n",
    "print(f\"Exact Match Ratio: {exact_match_ratio:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step\n"
     ]
    }
   ],
   "source": [
    "folder_path_test = r\"C:\\Users\\farel\\Documents\\Hology UB\\data\\HOLOGY 2024\\processed_test\"\n",
    "test_data = load_images_from_folder(folder_path_test)\n",
    "test_data = test_data.astype('float32') / 255.0\n",
    "\n",
    "# Make predictions on the test images\n",
    "predictions = model.predict(test_data)\n",
    "\n",
    "# If it's a classification model with softmax, get the class with the highest probability\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "output_df = pd.DataFrame({\n",
    "    'predictions': predicted_classes  # Only one column for the predictions\n",
    "})\n",
    "\n",
    "# Save to CSV (you can choose any filename)\n",
    "output_csv_path = r\"C:\\Users\\farel\\Documents\\Hology UB\\data\\HOLOGY 2024\\test_predictions.csv\"\n",
    "output_df.to_csv(output_csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 102ms/step\n",
      "Predictions saved to C:\\Users\\farel\\Documents\\Hology UB\\data\\HOLOGY 2024\\test_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "\n",
    "# Function to load images using filenames in the test folder\n",
    "def load_test_images(folder_path, target_size=(200, 200)):\n",
    "    images = []\n",
    "    image_ids = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        img_path = os.path.join(folder_path, filename)\n",
    "        try:\n",
    "            # Open image and resize it to match the model input\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "            img = img.resize(target_size)\n",
    "            img_array = np.array(img)\n",
    "            images.append(img_array)\n",
    "\n",
    "            # Extract image id (assuming filenames are 'id.jpg')\n",
    "            image_id = os.path.splitext(filename)[0]\n",
    "            image_ids.append(image_id)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load image {filename}: {e}\")\n",
    "    \n",
    "    return np.array(images), image_ids\n",
    "\n",
    "# Assuming your test folder path\n",
    "test_folder = r\"C:\\Users\\farel\\Documents\\Hology UB\\data\\HOLOGY 2024\\processed_test\"\n",
    "\n",
    "# Load the test images and their corresponding IDs\n",
    "test_images, test_image_ids = load_test_images(test_folder)\n",
    "\n",
    "# Normalize the test images (assuming your model expects [0, 1] range)\n",
    "test_images = test_images.astype('float32') / 255.0\n",
    "\n",
    "# Make predictions using the trained model\n",
    "predictions = model.predict(test_images)\n",
    "\n",
    "# Assuming the model outputs probabilities for a multi-class classification task\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Create a DataFrame with 'id' and 'predicted_class' columns\n",
    "output_df = pd.DataFrame({\n",
    "    'id': test_image_ids,\n",
    "    'predicted_class': predicted_classes\n",
    "})\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "output_csv_path = r\"C:\\Users\\farel\\Documents\\Hology UB\\data\\HOLOGY 2024\\test_predictions.csv\"\n",
    "output_df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "print(f\"Predictions saved to {output_csv_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['id'] = submission['id'].astype(str)\n",
    "output_df['id'] = output_df['id'].astype(str)\n",
    "\n",
    "\n",
    "updated_submission_df = submission.merge(output_df[['id', 'predicted_class']], on='id', how='left')\n",
    "\n",
    "# Step 3: Replace the 'label' column in submission_df with 'predicted_class'\n",
    "updated_submission_df['jenis'] = updated_submission_df['predicted_class']\n",
    "\n",
    "# Step 4: Drop the 'predicted_class' column as it was only for merging\n",
    "updated_submission_df = updated_submission_df.drop(columns=['predicted_class'])\n",
    "\n",
    "# Step 5: Save the updated submission file\n",
    "updated_submission_path = r\"C:\\Users\\farel\\Documents\\Hology UB\\data\\HOLOGY 2024\\updated_submission.csv\"\n",
    "updated_submission_df.to_csv(updated_submission_path, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
