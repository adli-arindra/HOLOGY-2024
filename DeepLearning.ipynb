{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Function to load and preprocess the images\n",
    "def load_images_from_folder(folder_path, target_size=(200, 200)):\n",
    "    images = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        img_path = os.path.join(folder_path, filename)\n",
    "        try:\n",
    "            # Open image and resize it\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "            img = img.resize(target_size)  # Resizing the image to a target size\n",
    "            img_array = np.array(img)  # Converting to NumPy array\n",
    "            images.append(img_array)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to process {img_path}: {e}\")\n",
    "    \n",
    "    images = np.array(images)  # Convert list of images to a NumPy array\n",
    "    return images\n",
    "\n",
    "# Specify folder path and target image size\n",
    "folder_path = \"processed/\"\n",
    "images_data = load_images_from_folder(folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\farel\\\\Documents\\\\Hology UB\\\\data\\\\train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [3], line 25\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Load the CSV file\u001b[39;00m\n\u001b[0;32m     24\u001b[0m csv_file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mfarel\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDocuments\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mHology UB\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mtrain.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 25\u001b[0m labels_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_file_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Get the 'id' and 'label' columns from the CSV\u001b[39;00m\n\u001b[0;32m     28\u001b[0m image_ids \u001b[38;5;241m=\u001b[39m labels_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues  \u001b[38;5;66;03m# The 'id' column from CSV\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Adli\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Adli\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Adli\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Adli\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\Adli\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Adli\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1738\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1742\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1744\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\Adli\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    851\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    852\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    853\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    855\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    857\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    858\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    859\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\farel\\\\Documents\\\\Hology UB\\\\data\\\\train.csv'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "\n",
    "# Function to load images using the 'id' from the CSV\n",
    "def load_images_from_ids(folder_path, ids, target_size=(200, 200)):\n",
    "    images = []\n",
    "    for image_id in ids:\n",
    "        img_filename = f\"{image_id}.jpg\"  # Assuming image filenames are of the format 'id.jpg'\n",
    "        img_path = os.path.join(folder_path, img_filename)\n",
    "        try:\n",
    "            # Open image, resize, and convert to NumPy array\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "            img = img.resize(target_size)\n",
    "            img_array = np.array(img)\n",
    "            images.append(img_array)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load image {img_filename}: {e}\")\n",
    "    \n",
    "    return np.array(images)\n",
    "\n",
    "# Load the CSV file\n",
    "csv_file_path = r\"C:\\Users\\farel\\Documents\\Hology UB\\data\\train.csv\"\n",
    "labels_df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Get the 'id' and 'label' columns from the CSV\n",
    "image_ids = labels_df['id'].values  # The 'id' column from CSV\n",
    "jenis_labels = labels_df['jenis'].values  # The 'label' column from CSV\n",
    "warna_labels = labels_df['warna'].values  # The 'label' column from CSV\n",
    "\n",
    "# Folder where images are stored\n",
    "folder_path = r\"C:\\Users\\farel\\Documents\\Hology UB\\data\\processed\"\n",
    "\n",
    "# Load images based on 'id' column\n",
    "images_data = load_images_from_ids(folder_path, image_ids)\n",
    "# images_data = images_data.astype('float32') / 255.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "# Step 1: Label encode the categorical labels into integers\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(jenis_labels)  # 'labels' should come from your CSV\n",
    "\n",
    "# Step 2: Apply OneHotEncoder to the encoded labels\n",
    "one_hot_encoder = OneHotEncoder(sparse_output=False)\n",
    "one_hot_labels = one_hot_encoder.fit_transform(encoded_labels.reshape(-1, 1))\n",
    "\n",
    "label_encoder2 = LabelEncoder()\n",
    "encoded_labels2 = label_encoder2.fit_transform(warna_labels)  # 'labels' should come from your CSV\n",
    "\n",
    "# Step 2: Apply OneHotEncoder to the encoded labels\n",
    "one_hot_encoder2 = OneHotEncoder(sparse_output=False)\n",
    "one_hot_labels2 = one_hot_encoder2.fit_transform(encoded_labels2.reshape(-1, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Normalize the image data to [0, 1] range\n",
    "images_data = images_data.astype('float32') / 255.0\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train1, y_test1, y_train2, y_test2 = train_test_split(\n",
    "    images_data, one_hot_labels, one_hot_labels2, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert the arrays to TensorFlow tensors\n",
    "X_train = tf.convert_to_tensor(X_train)\n",
    "X_test = tf.convert_to_tensor(X_test)\n",
    "y_train1 = tf.convert_to_tensor(y_train1)\n",
    "y_train2 = tf.convert_to_tensor(y_train2)\n",
    "y_test1 = tf.convert_to_tensor(y_test1)\n",
    "y_test2 = tf.convert_to_tensor(y_test2)\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train1 shape after conversion: (621,)\n",
      "y_train2 shape after encoding: (621, 5)\n"
     ]
    }
   ],
   "source": [
    "# Convert one-hot encoded binary labels to single binary values\n",
    "if y_train1.ndim > 1:  # If y_train1 is one-hot encoded\n",
    "    y_train1 = np.argmax(y_train1, axis=1)  # Convert to binary labels\n",
    "\n",
    "if y_test1.ndim > 1:  # Same for test data\n",
    "    y_test1 = np.argmax(y_test1, axis=1)\n",
    "\n",
    "print(\"y_train1 shape after conversion:\", y_train1.shape)  # Should be (batch_size,)\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# If y_train2 is not already one-hot encoded, one-hot encode it\n",
    "if y_train2.ndim == 1:  # If y_train2 is class indices (shape: (batch_size,))\n",
    "    y_train2 = to_categorical(y_train2, num_classes=5)  # Assuming 5 classes\n",
    "    y_test2 = to_categorical(y_test2, num_classes=5)\n",
    "\n",
    "print(\"y_train2 shape after encoding:\", y_train2.shape)  # Should be (batch_size, num_classes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 829ms/step - loss: 4.7588 - output1_accuracy: 0.5532 - output1_loss: 1.4523 - output2_accuracy: 0.2991 - output2_loss: 3.1871 - val_loss: 1.6450 - val_output1_accuracy: 0.6410 - val_output1_loss: 0.6764 - val_output2_accuracy: 0.7692 - val_output2_loss: 0.7985\n",
      "Epoch 2/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 769ms/step - loss: 1.8915 - output1_accuracy: 0.5629 - output1_loss: 0.7904 - output2_accuracy: 0.6616 - output2_loss: 0.9206 - val_loss: 1.2458 - val_output1_accuracy: 0.6538 - val_output1_loss: 0.6030 - val_output2_accuracy: 0.8910 - val_output2_loss: 0.4558\n",
      "Epoch 3/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 754ms/step - loss: 1.5095 - output1_accuracy: 0.6469 - output1_loss: 0.7071 - output2_accuracy: 0.8079 - output2_loss: 0.6111 - val_loss: 1.0757 - val_output1_accuracy: 0.7821 - val_output1_loss: 0.5189 - val_output2_accuracy: 0.9231 - val_output2_loss: 0.3714\n",
      "Epoch 4/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 763ms/step - loss: 1.3208 - output1_accuracy: 0.6872 - output1_loss: 0.6123 - output2_accuracy: 0.8267 - output2_loss: 0.5201 - val_loss: 0.9316 - val_output1_accuracy: 0.8077 - val_output1_loss: 0.4922 - val_output2_accuracy: 0.9359 - val_output2_loss: 0.2555\n",
      "Epoch 5/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 767ms/step - loss: 1.1541 - output1_accuracy: 0.7068 - output1_loss: 0.5773 - output2_accuracy: 0.8621 - output2_loss: 0.3862 - val_loss: 1.1276 - val_output1_accuracy: 0.8013 - val_output1_loss: 0.4798 - val_output2_accuracy: 0.7628 - val_output2_loss: 0.4549\n",
      "Epoch 6/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 744ms/step - loss: 1.1357 - output1_accuracy: 0.7400 - output1_loss: 0.5455 - output2_accuracy: 0.8623 - output2_loss: 0.3937 - val_loss: 0.8547 - val_output1_accuracy: 0.8526 - val_output1_loss: 0.4419 - val_output2_accuracy: 0.9295 - val_output2_loss: 0.2200\n",
      "Epoch 7/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 768ms/step - loss: 0.9530 - output1_accuracy: 0.7877 - output1_loss: 0.4276 - output2_accuracy: 0.8822 - output2_loss: 0.3287 - val_loss: 0.8263 - val_output1_accuracy: 0.8590 - val_output1_loss: 0.4415 - val_output2_accuracy: 0.9551 - val_output2_loss: 0.1943\n",
      "Epoch 8/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 780ms/step - loss: 0.7986 - output1_accuracy: 0.8312 - output1_loss: 0.3789 - output2_accuracy: 0.9306 - output2_loss: 0.2259 - val_loss: 0.7836 - val_output1_accuracy: 0.8718 - val_output1_loss: 0.4289 - val_output2_accuracy: 0.9359 - val_output2_loss: 0.1721\n",
      "Epoch 9/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 783ms/step - loss: 0.8133 - output1_accuracy: 0.8194 - output1_loss: 0.3772 - output2_accuracy: 0.9080 - output2_loss: 0.2490 - val_loss: 0.7880 - val_output1_accuracy: 0.8590 - val_output1_loss: 0.4167 - val_output2_accuracy: 0.9359 - val_output2_loss: 0.1901\n",
      "Epoch 10/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 805ms/step - loss: 0.8399 - output1_accuracy: 0.8171 - output1_loss: 0.3498 - output2_accuracy: 0.9046 - output2_loss: 0.3038 - val_loss: 0.8490 - val_output1_accuracy: 0.8269 - val_output1_loss: 0.4480 - val_output2_accuracy: 0.9167 - val_output2_loss: 0.2203\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, MaxPooling2D, Input, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "\n",
    "# Input layer\n",
    "input_layer = Input(shape=(200, 200, 3))\n",
    "\n",
    "# Shared convolutional base\n",
    "x = Conv2D(64, (3, 3), activation='relu')(input_layer)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.001))(x)\n",
    "x = Dropout(0.6)(x)  \n",
    "\n",
    "# Output 1: Binary classification\n",
    "output1 = Dense(1, activation='sigmoid', name='output1')(x)\n",
    "\n",
    "# Output 2: Multi-class classification (5 classes)\n",
    "output2 = Dense(5, activation='softmax', name='output2')(x)\n",
    "\n",
    "# Define the model with two outputs\n",
    "model = Model(inputs=input_layer, outputs=[output1, output2])\n",
    "\n",
    "# Compile the model with accuracy as the metric\n",
    "model.compile(optimizer='adam',\n",
    "              loss={'output1': 'binary_crossentropy', 'output2': 'categorical_crossentropy'},\n",
    "              metrics={'output1': 'accuracy', 'output2': 'accuracy'})\n",
    "\n",
    "# Early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2, min_lr=0.00001)\n",
    "\n",
    "\n",
    "\n",
    "# Train the model with both outputs\n",
    "history = model.fit(X_train, {'output1': y_train1, 'output2': y_train2},\n",
    "                    validation_data=(X_test, {'output1': y_test1, 'output2': y_test2}),\n",
    "                    epochs=10, batch_size=32, callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 204ms/step\n",
      "Exact Match Ratio: 0.8269\n"
     ]
    }
   ],
   "source": [
    "# Get predictions from the model on the test data\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Split predictions for both outputs\n",
    "pred_output1 = predictions[0]  # Binary classification predictions\n",
    "pred_output2 = predictions[1]  # Multi-class classification predictions\n",
    "# Convert binary predictions to 0 or 1\n",
    "pred_output1_binary = np.round(pred_output1).astype(int)\n",
    "\n",
    "# Convert multi-class predictions to class indices (e.g., choose the class with the highest probability)\n",
    "pred_output2_classes = np.argmax(pred_output2, axis=1)\n",
    "\n",
    "# Now compare with true labels (already converted earlier)\n",
    "correct_binary = (pred_output1_binary.squeeze() == y_test1).astype(int)\n",
    "correct_multiclass = (pred_output2_classes == np.argmax(y_test2, axis=1)).astype(int)\n",
    "\n",
    "# Exact match: Both binary and multi-class predictions must be correct for each sample\n",
    "exact_matches = (correct_binary & correct_multiclass)\n",
    "\n",
    "# Calculate exact match ratio\n",
    "exact_match_ratio = np.mean(exact_matches)\n",
    "\n",
    "print(f\"Exact Match Ratio: {exact_match_ratio:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step\n"
     ]
    }
   ],
   "source": [
    "folder_path_test = r\"C:\\Users\\farel\\Documents\\Hology UB\\data\\HOLOGY 2024\\processed_test\"\n",
    "test_data = load_images_from_folder(folder_path_test)\n",
    "test_data = test_data.astype('float32') / 255.0\n",
    "\n",
    "# Make predictions on the test images\n",
    "predictions = model.predict(test_data)\n",
    "\n",
    "# If it's a classification model with softmax, get the class with the highest probability\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "output_df = pd.DataFrame({\n",
    "    'predictions': predicted_classes  # Only one column for the predictions\n",
    "})\n",
    "\n",
    "# Save to CSV (you can choose any filename)\n",
    "output_csv_path = r\"C:\\Users\\farel\\Documents\\Hology UB\\data\\HOLOGY 2024\\test_predictions.csv\"\n",
    "output_df.to_csv(output_csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 102ms/step\n",
      "Predictions saved to C:\\Users\\farel\\Documents\\Hology UB\\data\\HOLOGY 2024\\test_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "\n",
    "# Function to load images using filenames in the test folder\n",
    "def load_test_images(folder_path, target_size=(200, 200)):\n",
    "    images = []\n",
    "    image_ids = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        img_path = os.path.join(folder_path, filename)\n",
    "        try:\n",
    "            # Open image and resize it to match the model input\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "            img = img.resize(target_size)\n",
    "            img_array = np.array(img)\n",
    "            images.append(img_array)\n",
    "\n",
    "            # Extract image id (assuming filenames are 'id.jpg')\n",
    "            image_id = os.path.splitext(filename)[0]\n",
    "            image_ids.append(image_id)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load image {filename}: {e}\")\n",
    "    \n",
    "    return np.array(images), image_ids\n",
    "\n",
    "# Assuming your test folder path\n",
    "test_folder = r\"C:\\Users\\farel\\Documents\\Hology UB\\data\\HOLOGY 2024\\processed_test\"\n",
    "\n",
    "# Load the test images and their corresponding IDs\n",
    "test_images, test_image_ids = load_test_images(test_folder)\n",
    "\n",
    "# Normalize the test images (assuming your model expects [0, 1] range)\n",
    "test_images = test_images.astype('float32') / 255.0\n",
    "\n",
    "# Make predictions using the trained model\n",
    "predictions = model.predict(test_images)\n",
    "\n",
    "# Assuming the model outputs probabilities for a multi-class classification task\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Create a DataFrame with 'id' and 'predicted_class' columns\n",
    "output_df = pd.DataFrame({\n",
    "    'id': test_image_ids,\n",
    "    'predicted_class': predicted_classes\n",
    "})\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "output_csv_path = r\"C:\\Users\\farel\\Documents\\Hology UB\\data\\HOLOGY 2024\\test_predictions.csv\"\n",
    "output_df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "print(f\"Predictions saved to {output_csv_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['id'] = submission['id'].astype(str)\n",
    "output_df['id'] = output_df['id'].astype(str)\n",
    "\n",
    "\n",
    "updated_submission_df = submission.merge(output_df[['id', 'predicted_class']], on='id', how='left')\n",
    "\n",
    "# Step 3: Replace the 'label' column in submission_df with 'predicted_class'\n",
    "updated_submission_df['jenis'] = updated_submission_df['predicted_class']\n",
    "\n",
    "# Step 4: Drop the 'predicted_class' column as it was only for merging\n",
    "updated_submission_df = updated_submission_df.drop(columns=['predicted_class'])\n",
    "\n",
    "# Step 5: Save the updated submission file\n",
    "updated_submission_path = r\"C:\\Users\\farel\\Documents\\Hology UB\\data\\HOLOGY 2024\\updated_submission.csv\"\n",
    "updated_submission_df.to_csv(updated_submission_path, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
